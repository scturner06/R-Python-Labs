{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3.1 The Validation Set Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we’ll explore the use of the validation set approach in order to estimate the test error rates that result from fitting various linear models on the Auto data set.\n",
    "\n",
    "After loading the data, we'll use the `radom_state` parameter in the `train_test_split()` function from the `sklearn` pacakge in order to set a seed for random number generator, so that you’ll obtain precisely the same results as those shown in the textbook. It is generally a good idea to set a random seed when performing an analysis such as cross-validation that contains an element of randomness, so that the results obtained can be reproduced precisely at a later time.\n",
    "\n",
    "We begin by using the `train_test_split()` functions to split the set of observations into two halves. We’ll start by selecting a random subset of 196 observations out of the original 392 observations. We refer to these observations as the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "autoData = pd.read_csv(\"../../data/Auto.csv\", index_col = None, na_values = ['?'])  # data contains ? for NA values\n",
    "autoData = autoData.dropna() # remove NA since for regression\n",
    "\n",
    "train, test = train_test_split(autoData, test_size = 0.5, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then make a `LinearRegression` object to access methods from the `sklearn` package in SciPy to fit a linear regression using only the observations corresponding to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model_LR = LinearRegression()\n",
    "model_LR.fit(train[['horsepower']], train[['mpg']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the `predict` method to estimate the response for the test observations, and we use the `mean()` function to calculate the MSE of the 196 observations in the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mpg    24.802121\n",
       "dtype: float64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((model_LR.predict(test[['horsepower']]) - test[['mpg']]) ** 2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the estimated test MSE for the linear regression fit is `24.80`.  We can now create an object to perfom a quadradic and cubic regression by utilzing the `PolynomialFeatures` and `Pipeline` packages from `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First on a quadradic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mpg    18.848293\n",
       "dtype: float64"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_QUAD = Pipeline([('poly', PolynomialFeatures(degree = 2)),\n",
    "                       ('linear', LinearRegression(fit_intercept = False))]) # create a polynomial object\n",
    "model_QUAD.fit(train[['horsepower']], train[['mpg']])\n",
    "\n",
    "((model_QUAD.predict(test[['horsepower']]) - test[['mpg']]) ** 2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then a cubic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mpg    18.805111\n",
       "dtype: float64"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_CUBIC = Pipeline([('poly', PolynomialFeatures(degree = 3)),\n",
    "                       ('linear', LinearRegression(fit_intercept = False))])\n",
    "model_CUBIC.fit(train[['horsepower']], train[['mpg']])\n",
    "\n",
    "((model_CUBIC.predict(test[['horsepower']]) - test[['mpg']]) ** 2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These error rates are `18.85` and `18.81`, respectively. If we choose a different training set instead, then we will obtain somewhat different errors on the validation set. We can test this out by setting a different random seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(autoData, test_size = 0.5, random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mpg    23.442644\n",
       "dtype: float64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_LR2 = LinearRegression()\n",
    "model_LR2.fit(train[['horsepower']], train[['mpg']])\n",
    "\n",
    "((model_LR2.predict(test[['horsepower']]) - test[['mpg']]) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mpg    18.550199\n",
       "dtype: float64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_QUAD2 = Pipeline([('poly', PolynomialFeatures(degree = 2)),\n",
    "                       ('linear', LinearRegression(fit_intercept = False))])\n",
    "model_QUAD2.fit(train[['horsepower']], train[['mpg']])\n",
    "\n",
    "((model_QUAD2.predict(test[['horsepower']]) - test[['mpg']]) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mpg    18.595222\n",
       "dtype: float64"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_CUBIC2 = Pipeline([('poly', PolynomialFeatures(degree = 3)),\n",
    "                       ('linear', LinearRegression(fit_intercept = False))])\n",
    "model_CUBIC2.fit(train[['horsepower']], train[['mpg']])\n",
    "\n",
    "((model_CUBIC2.predict(test[['horsepower']]) - test[['mpg']]) ** 2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this split of the observations into a training set and a validation set, we find that the validation set error rates for the models with linear, quadratic, and cubic terms are `23.44`, `18.55`, and `18.60`, respectively.\n",
    "\n",
    "These results are consistent with our previous findings: a model that predicts mpg using a quadratic function of horsepower performs better than a model that involves only a linear function of horsepower, and there is little evidence in favor of a model that uses a cubic function of horsepower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3.2 Leave-One-Out Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LOOCV estimate can be automatically computed for any generalized linear model using the `cross_val_score()` and `cross_val_predict()` functions from the `sklearn` package.  These two functions can be tuned for various sampling and scoring methods depending on your needs.  For the example below, we will use the `cross_val_predict()` function to perform LOOCV, obtaining prediction values and then compute the mean square erorr of the LOOCV using the `mean_square_error()` function also from `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then perform the cross validation using the `cross_val_predict()` function.  For LOOCV we will first pass the function our `LinearRegression` model object from above, set the `cv` paramater equal to the number of observations in our dataset for LOOCV, and then use `mean_square_error()` function to compute a cross validation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.23151351792923"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = cross_val_predict(model_LR, autoData[['horsepower']],\n",
    "                         autoData[['mpg']],\n",
    "                         cv = len(autoData[['mpg']]))\n",
    "\n",
    "mean_squared_error(autoData[['mpg']], predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the prediction error is `24.23`.  This isn't too useful unless it is used to compare several different models which we will do below.\n",
    "\n",
    "We can repeat this procedure for increasingly complex polynomial fits. To automate the process, we use the for loop which iteratively fits polynomial regressions for polynomials of order 1 to 5 and computes the associated cross-validation error.\n",
    "\n",
    "This command may take a couple of minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24.231513517929233,\n",
       " 19.248213124490388,\n",
       " 19.334984064020858,\n",
       " 19.424430299871521,\n",
       " 19.03326050458827]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_scores = [];\n",
    "for i in range(1, 6):\n",
    "    model_GLR = Pipeline([('poly', PolynomialFeatures(degree = i)),\n",
    "                          ('linear', LinearRegression(fit_intercept = False))])\n",
    "    model_GLR.fit(autoData[['horsepower']], autoData[['mpg']])\n",
    "    \n",
    "    pred = cross_val_predict(model_GLR, autoData[['horsepower']],\n",
    "                             autoData[['mpg']],\n",
    "                             cv = len(autoData[['horsepower']]))\n",
    "    \n",
    "    model_scores.append(mean_squared_error(autoData[['mpg']], pred))\n",
    "    \n",
    "model_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see a sharp drop in the estimated test MSE between the linear and quadratic fits, but then no clear improvement from using higher-order polynomials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3.3 k-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cross_val_predict()` and `cross_val_score()` functions can also be used to implement k-Fold CV. Above, we set the `cv` parameter equal to the number of observations in order to perform LOOCV.  Below we use k = 10, a common choice for k, on the Auto data set. We once again set a random seed and initialize a vector in which we will store the CV errors corresponding to the polynomial fits of orders one to ten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27.416194818355972,\n",
       " 21.202293642897722,\n",
       " 21.302479720296542,\n",
       " 21.319376801860198,\n",
       " 20.869201167625281,\n",
       " 20.865450400076767,\n",
       " 20.918115351773562,\n",
       " 25.610794730521821,\n",
       " 42.800730506916601,\n",
       " 84.280038134738064]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_scores = [];\n",
    "for i in range(1, 11):\n",
    "    model_GLR = Pipeline([('poly', PolynomialFeatures(degree = i)),\n",
    "                          ('linear', LinearRegression(fit_intercept = False))])\n",
    "    model_GLR.fit(autoData[['horsepower']], autoData[['mpg']])\n",
    "    \n",
    "    pred = cross_val_predict(model_GLR, autoData[['horsepower']],\n",
    "                             autoData[['mpg']],\n",
    "                             cv = 10)\n",
    "    \n",
    "    model_scores.append(mean_squared_error(autoData[['mpg']], pred))\n",
    "    \n",
    "model_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the computation time is much shorter than that of LOOCV.  We still see little evidence that using cubic or higher-order polynomial terms leads to lower test error than simply using a quadratic fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Application to Default Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you’re armed with more useful technique for resampling your data, let’s try fitting a model for the Default dataset.  Before fitting, we'll need to import the data and convert to dummy variables in order to use the functions in the `sklearn` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "defaultData = pd.read_csv('../../data/Default.csv', index_col = 0)\n",
    "\n",
    "enc = LabelEncoder()\n",
    "defaultData.default = enc.fit_transform(defaultData.default)\n",
    "defaultData.student = enc.fit_transform(defaultData.student)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we’ll try just holding out a random 20% of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cMatrix_LG = []\n",
    "score_LG = []\n",
    "\n",
    "for i in range(0, 10):\n",
    "    train, test = train_test_split(defaultData, test_size = 0.2, random_state = i)\n",
    "    \n",
    "    # Fit the logistic model\n",
    "    model_LG = LogisticRegression()\n",
    "    model_LG.fit(train[['balance', 'student']], train['default'])\n",
    "    \n",
    "    # Use the model to predict the response\n",
    "    pred_LG = model_LG.predict(test[['balance', 'student']])\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cMatrix_LG.append(confusion_matrix(test['default'], pred_LG))\n",
    "    \n",
    "    # Prediction Scores\n",
    "    score_LG.append(model_LG.score(test[['balance', 'student']], test['default']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now print the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1924    2]\n",
      " [  56   18]]\n",
      "[[1932    9]\n",
      " [  43   16]]\n",
      "[[1933   10]\n",
      " [  43   14]]\n",
      "[[1939    2]\n",
      " [  39   20]]\n",
      "[[1928    3]\n",
      " [  47   22]]\n",
      "[[1930    4]\n",
      " [  49   17]]\n",
      "[[1931    3]\n",
      " [  52   14]]\n",
      "[[1929    4]\n",
      " [  47   20]]\n",
      "[[1933    2]\n",
      " [  49   16]]\n",
      "[[1944    3]\n",
      " [  36   17]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(cMatrix_LG)):\n",
    "    print(cMatrix_LG[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.97099999999999997,\n",
       " 0.97399999999999998,\n",
       " 0.97350000000000003,\n",
       " 0.97950000000000004,\n",
       " 0.97499999999999998,\n",
       " 0.97350000000000003,\n",
       " 0.97250000000000003,\n",
       " 0.97450000000000003,\n",
       " 0.97450000000000003,\n",
       " 0.98050000000000004]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_LG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy is really high on this data, but we’re getting different error rates depending on how we choose our test set. That’s no good!\n",
    "\n",
    "Unfortunately this dataset is too big for us to run LOOCV, so we’ll have to settle for k-fold. In the space below, build a logistic model on the full `Default` dataset and then run 5-fold cross-validation to get a more accurate estimate of your test error rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3.4 The Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We illustrate the use of the bootstrap in the simple example of Section 5.2, as well as on an example involving estimating the accuracy of the linear regression model on the `Auto` data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the Accuracy of a Statistic of Interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the great advantages of the bootstrap approach is that it can be applied in almost all situations. No complicated mathematical calculations are required. Performing a bootstrap analysis in Pyhton entails writing a function that will randomly sample you data, with or without replacement, and compute the statistics of interest.\n",
    "\n",
    "The `Portfolio` data set in the `ISLR` package is described in Section 5.2. To illustrate the use of the bootstrap on this data, we must first create a function, `alpha_boot()`, which takes as input the `(X,Y)` data as well as the number of times to resample and compute α. The function then outputs the estimates for α based on the selected observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will read in the `Portfolio` data and import the `numpy` package as we will need some of its functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "dataPort = pd.read_csv('../../data/Portfolio.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def alpha_boot(data, n):\n",
    "    alpha = [] # empty array to hold statistics from each iteration\n",
    "    for i in range(0, n):\n",
    "        dataSample = data.sample(len(data), replace = True) #sample data with replacement\n",
    "        x = dataSample['X']\n",
    "        y = dataSample['Y']\n",
    "        alpha.append((np.var(y) - np.cov(x, y)[0, 1]) / (np.var(x) + np.var(y)- 2 * np.cov(x, y)[0, 1]))\n",
    "    return(alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the `alpha_boot()` function for 1000 iterations with a sample size of 100 and compute the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.58207219281140044"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha1 = alpha_boot(dataPort, 1000)\n",
    "np.mean(alpha1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also of interest may be the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.090831910379228983"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(alpha1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the Accuracy of a Linear Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bootstrap approach can be used to assess the variability of the coefficient estimates and predictions from a statistical learning method. Here we use the bootstrap approach in order to assess the goodness of fit for the regression model to predict `mpg` as a function of `horsepower` in the `Auto` data set.\n",
    "\n",
    "We first create a simple function, `bootstrap_reg()`, which takes the data, number of bootstrap samples to take, and the degree of the polynomial to fit, and returns the square errors for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bootstrap_reg(data, n, deg):\n",
    "    error = []\n",
    "    for i in range(0, n):\n",
    "        model = Pipeline([('poly', PolynomialFeatures(degree = deg)),\n",
    "                          ('linear', LinearRegression(fit_intercept = False))])\n",
    "        dataSample = data.sample(len(data), replace = True)\n",
    "        model.fit(dataSample[[0]], dataSample[[1]])\n",
    "        error.append(((dataSample[[1]] - model.predict(dataSample[[0]])) ** 2).mean()[0])\n",
    "    return(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the function to perform a bootstrap on the `Auto` data set for a simple linear regression case.  Once we have the errors, we can compute some statistics to determine the quality of fit the same as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "bs_LR = bootstrap_reg(autoData, 1000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1429063902156222"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(bs_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.082023136952851947"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(bs_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we found previously that the relationship between `horsepower` and `mpg` is better characterized by a quadratic model. Let’s see how the error rates compare when we fit that instead of a linear model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bs_QUAD = bootstrap_reg(autoData, 1000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75115009414858103"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(bs_QUAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.076018747428914638"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(bs_QUAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, a quadradic provides a bit better fit than the linear model alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get credit for this lab, please post your answers to the following questions: - How did the cross-validated error rate compare to the models where you held out a validation set? Why do you think that is? - How do you explain the discrepancy between the bootstrap evaluation and the standard error evaluation of the linear model predicting `mpg` from `horsepower`? - What was the most confusing part of today’s class?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
