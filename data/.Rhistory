# values no larger than 1000 are recommended.
#
# Args:
#   line: line to be split into groups.
#   max.length: Maximum length of string to send to API to be translated.
#               Strings greater than this length will be split before passing
#               to API.  Absolute maximum is approximately 1400.
# Returns:
#   A list containing line split into groups of size about max.length
line.groups <- line %>%
strsplit("[.]") %>%  # Split by periods
unlist %>%
lapply(function(x) paste0(x, ".")) %>%
lapply(nchar) %>%  # List of counts of number of chars in each sentence
as.numeric %>%
cumsum %>%
`/`(max.length) %>%
floor %>%  # Above three lines split counts of chars into groups
cbind(paste0(unlist(strsplit(line, "[.]")), ".")) %>%  # Attach group numbers
as.data.frame %>%
setNames(c("grp", "text")) %>%
transform(text = as.character(text)
, grp = as.factor(as.character(grp)))
line.groups <- aggregate(line.groups, by = list(line.groups$grp)
, function(x) paste(x, collapse = ""))  # paste grps
return(line.groups$text)
}
library(translateR)
data(enron)
test <- gTranslate.df(enron$email, lang.out = "de")
test.all <- gTranslate(paste(enron$email, collapse = ""), lang.out = "de")
cnt.test <- lapply(test$translation, nchar) %>% unlist %>% sum
cnt.test.all <- nchar(test.all[1])
print.test <- lappy(test$translation, paste(collapse = ))
print.test <- lapply(test$translation, paste(collapse = ))
print.test <- lapply(test$translation, paste(collapse = ""))
print.test <- lapply(test$translation, paste, collapse = "")
print.test <- past(test$translation, collapse = "")
print.test <- paste(test$translation, collapse = "")
nchar(print.test)
print.test.all <- test.all[1]
write(print.test, "print_test1.txt")
write(print.test.all, "print_test2.txt")
# Author: Sam Turner
# Source file for functions that utilize google translate to translate text.
# Note that this does not use the standard google translate API which is a paid
# service.  It instead utilizes the API used by the google translate chrome
# extention.  It's unknown whether there are limits to the use of this service.
# It seems to be able to handle a maximum of around 1400 characters per request
# which omes out to about 311 English words.  This is likely due to URL request
# restrictions.  As a workaround, strings with a length greater than 1000 by
# default are split into smaller sets before passing to API.
# LIBRARIES
library(RJSONIO)  #jsonlite will not process query correctly
library(dplyr)
# URL
# Chrome translate extention base URL
url <- "https://translate.googleapis.com/translate_a/single?client=gtx&sl="
#FUNCTIONS
gTranslate <- function(line, lang.in = "auto", lang.out, max.length = 800) {
# Translates a line of text via api call.
#
# Args:
#   line: Line of text to be translated.
#   lang.in: Input language of text.  Default setting auto will automatically
#            detect language.
#   lang.out: Output langage of text.  See below URL for a list of lang codes
#             https://cloud.google.com/translate/docs/languages
#   max.length: Maximum length of string to send to API to be translated.
#               Strings greater than this length will be split before passing
#               to API.  Absolute maximum is approximately 1400.
# Returns:
#   A vector containing the translated text and the input language code
# Checks to see if line is greater than max allowed by API
if(nchar(line) > max.length) {
line <- gStringSplit(line, max.length)  # Split to smaller groups
}
# Builds a list of API calls and gets JSON for each translation
g.url <- lapply(line, function(x)
paste0(url, lang.in, "&tl=", lang.out, "&dt=t", "&q=", x))
g.JSON <- lapply(g.url, function(x) fromJSON(x, encoding = "UTF-8"))
# Reads through JSON, extracting the translated text and puts into a string
g.trans <- lapply(g.JSON, function(j)
lapply(j[[1]], function(x) return(x[1]))) %>%
unlist %>%
paste(collapse = "")
# Attach langauge code
g.lang <- ifelse(lang.in == "auto"
, g.JSON[[1]][[9]][[4]][1]
, lang.in)
return(c(g.trans, g.lang))
}
gTranslate.df <- function(v, lang.in = "auto", lang.out, max.length = 800) {
# Translates a vector or list of text via api call.
#
# Args:
#   v: Vector or list containing a text to translate in each row.
#   lang.in: Input language of text.  Default setting auto will automatically
#            detect language.
#   lang.out: Output langage of text.  See below URL for a list of lang codes
#             https://cloud.google.com/translate/docs/languages
#   max.length: Maximum length of string to send to API to be translated.
#               Strings greater than this length will be split before passing
#               to API.  Absolute maximum is approximately 1400.
# Returns:
#   A dataframe of the form original text, input language, translated text,
#   and output language.
# Applies gTranslate function to each row in vector and puts results into
# a dataframe
g.df <- lapply(v, function(r) gTranslate(r, lang.in, lang.out)) %>%
as.data.frame(stringsAsFactors = FALSE) %>%
t %>%
as.data.frame(stringsAsFactors = FALSE)
# Builds final dataframe, converting all cols to chars
g.df <- cbind(v, select(g.df, 2, 1), c(lang.out)) %>%
setNames(c("original", "lang.in", "translation", "lang.out")) %>%
transform(original = as.character(original)
, lang.out = as.character(lang.out))
rownames(g.df) <- NULL
return(g.df)
}
gStringSplit <- function(line, max.length = 800) {
# Function to split strings longer than max.length before passing to API
# NOTE: Split may be larger or smaller than max.lengh.  For this reason,
# values no larger than 1000 are recommended.
#
# Args:
#   line: line to be split into groups.
#   max.length: Maximum length of string to send to API to be translated.
#               Strings greater than this length will be split before passing
#               to API.  Absolute maximum is approximately 1400.
# Returns:
#   A list containing line split into groups of size about max.length
line.groups <- line %>%
strsplit("[.]") %>%  # Split by periods
unlist %>%
lapply(function(x) paste0(x, ".")) %>%
lapply(nchar) %>%  # List of counts of number of chars in each sentence
as.numeric %>%
cumsum %>%
`/`(max.length) %>%
floor %>%  # Above three lines split counts of chars into groups
cbind(paste0(unlist(strsplit(line, "[.]")), ".")) %>%  # Attach group numbers
as.data.frame %>%
setNames(c("grp", "text")) %>%
transform(text = as.character(text)
, grp = as.factor(as.character(grp)))
line.groups <- aggregate(line.groups, by = list(line.groups$grp)
, function(x) paste(x, collapse = ""))  # paste grps
return(line.groups$text)
}
library(translateR)
data(enron)
test <- gTranslate.df(enron$email, lang.out = "de")
test.all <- gTranslate(paste(enron$email, collapse = ""), lang.out = "de")
cnt.test <- lapply(test$translation, nchar) %>% unlist %>% sum
cnt.test.all <- nchar(test.all[1])
View(enron)
View(test)
# Author: Sam Turner
# Source file for functions that utilize google translate to translate text.
# Note that this does not use the standard google translate API which is a paid
# service.  It instead utilizes the API used by the google translate chrome
# extention.  It's unknown whether there are limits to the use of this service.
# It seems to be able to handle a maximum of around 1400 characters per request
# which omes out to about 311 English words.  This is likely due to URL request
# restrictions.  As a workaround, strings with a length greater than 800 by
# default are split into smaller sets before passing to API.
# LIBRARIES
library(RJSONIO)  #jsonlite will not process query correctly
library(dplyr)
# URL
# Chrome translate extention base URL
url <- "https://translate.googleapis.com/translate_a/single?client=gtx&sl="
#FUNCTIONS
gTranslate <- function(line, lang.in = "auto", lang.out, max.length = 800) {
# Translates a line of text via api call.
#
# Args:
#   line: Line of text to be translated.
#   lang.in: Input language of text.  Default setting auto will automatically
#            detect language.
#   lang.out: Output langage of text.  See below URL for a list of lang codes
#             https://cloud.google.com/translate/docs/languages
#   max.length: Maximum length of string to send to API to be translated.
#               Strings greater than this length will be split before passing
#               to API.  Absolute maximum is approximately 1400.
# Returns:
#   A vector containing the translated text and the input language code
# Checks to see if line is greater than max allowed by API
if(nchar(line) > max.length) {
line <- gStringSplit(line, max.length)  # Split to smaller groups
}
# Builds a list of API calls and gets JSON for each translation
g.url <- lapply(line, function(x)
paste0(url, lang.in, "&tl=", lang.out, "&dt=t", "&q=", x))
g.JSON <- lapply(g.url, function(x) fromJSON(x, encoding = "UTF-8"))
# Reads through JSON, extracting the translated text and puts into a string
g.trans <- lapply(g.JSON, function(j)
lapply(j[[1]], function(x) return(x[1]))) %>%
unlist %>%
paste(collapse = "")
# Attach langauge code
g.lang <- ifelse(lang.in == "auto"
, g.JSON[[1]][[9]][[4]][1]
, lang.in)
return(c(g.trans, g.lang))
}
gTranslate.df <- function(v, lang.in = "auto", lang.out, max.length = 800) {
# Translates a vector or list of text via api call.
#
# Args:
#   v: Vector or list containing a text to translate in each row.
#   lang.in: Input language of text.  Default setting auto will automatically
#            detect language.
#   lang.out: Output langage of text.  See below URL for a list of lang codes
#             https://cloud.google.com/translate/docs/languages
#   max.length: Maximum length of string to send to API to be translated.
#               Strings greater than this length will be split before passing
#               to API.  Absolute maximum is approximately 1400.
# Returns:
#   A dataframe of the form original text, input language, translated text,
#   and output language.
# Applies gTranslate function to each row in vector and puts results into
# a dataframe
g.df <- lapply(v, function(r) gTranslate(r, lang.in, lang.out)) %>%
as.data.frame(stringsAsFactors = FALSE) %>%
t %>%
as.data.frame(stringsAsFactors = FALSE)
# Builds final dataframe, converting all cols to chars
g.df <- cbind(v, select(g.df, 2, 1), c(lang.out)) %>%
setNames(c("original", "lang.in", "translation", "lang.out")) %>%
transform(original = as.character(original)
, lang.out = as.character(lang.out))
rownames(g.df) <- NULL
return(g.df)
}
gStringSplit <- function(line, max.length = 800) {
# Function to split strings longer than max.length before passing to API
# NOTE: Split may be larger or smaller than max.lengh.  For this reason,
# values no larger than 800 are recommended.
#
# Args:
#   line: line to be split into groups.
#   max.length: Maximum length of string to send to API to be translated.
#               Strings greater than this length will be split before passing
#               to API.  Absolute maximum is approximately 1400.
# Returns:
#   A list containing line split into groups of size about max.length
line.groups <- line %>%
strsplit("[.]") %>%  # Split by periods
unlist %>%
lapply(function(x) paste0(x, ".")) %>%
lapply(nchar) %>%  # List of counts of number of chars in each sentence
as.numeric %>%
cumsum %>%
`/`(max.length) %>%
floor %>%  # Above three lines split counts of chars into groups
cbind(paste0(unlist(strsplit(line, "[.]")), ".")) %>%  # Attach group numbers
as.data.frame %>%
setNames(c("grp", "text")) %>%
transform(text = as.character(text)
, grp = as.factor(as.character(grp)))
line.groups <- aggregate(line.groups, by = list(line.groups$grp)
, function(x) paste(x, collapse = ""))  # paste grps
return(line.groups$text)
}
library(translateR)
data(enron)
test.data <- gTranslate.df(test.data, lang.out = "fr")
test.data <- gTranslate.df(enron$email, lang.out = "fr")
View(test.data)
test.data <- gTranslate.df(test.data$translation, lang.out = "en")
test.data$translation[1]
library(ISLR)
install.packages("ISLR")
library(ISLR)
library(dplyr)
library(readr)
library(ggplot2)
library(GGally)
install.packages("GGally")
install.packages("mosaic")
install.packages("manipulate")
library(ISLR)
library(dplyr)
library(readr)
library(ggplot2)
library(GGally)
library(mosaic)
library(manipulate)
data(Auto)
data(Auto)
Auto$mpg
?slice
unlink('GitHub/R-Python-Labs/R/lab1_cache', recursive = TRUE)
Auto
Auto %>%
slice(1:10)
require(knitr)
opts_chunk$set(eval=FALSE)
library(ISLR)
library(dplyr)
library(readr)
library(ggplot2)
library(GGally)
library(mosaic)
library(manipulate)
data(Auto)
Auto
Auto %>%
slice(1:10)
Auto=read_csv("Auto.csv", na="?")
library(ISLR)
library(dplyr)
library(readr)
library(ggplot2)
library(GGally)
library(mosaic)
library(manipulate)
data(Auto)
Auto
Auto %>%
slice(1:10)
require(knitr)
opts_chunk$set(eval=FALSE)
library(ISLR)
library(dplyr)
library(readr)
library(ggplot2)
library(GGally)
library(mosaic)
library(manipulate)
data(Auto)
Auto
Auto %>%
slice(1:10)
#Auto=read_csv("Auto.csv", na="?")
#Auto %>%
#    slice(1:10)
dim(Auto)
str(Auto)
names(Auto)
Auto = Auto %>%
mutate(origin = factor(origin))
summary(Auto)
favstats(~mpg, data=Auto)
ggplot(Auto) + geom_boxplot(aes(x=cylinders, y=mpg)) + xlab("Cylinders") + ylab("MPG")
ggplot(Auto) + geom_point(aes(x=cylinders, y=mpg))
Auto = Auto %>%
mutate(cylinders = factor(cylinders))
ggplot(Auto) + geom_boxplot(aes(x=cylinders, y=mpg)) + xlab("Cylinders") + ylab("MPG")
require(knitr)
opts_chunk$set(eval=TRUE)
library(ISLR)
library(dplyr)
set.seed(1)
train = Auto %>%
sample_n(196)
test = Auto %>%
setdiff(train)
model_LR = lm(mpg~horsepower, data=train)
mean((test$mpg - predict(model_LR, test))^2)
model_QUAD = lm(mpg~poly(horsepower,2), data=train)
mean((test$mpg - predict(model_QUAD, test))^2)
model_CUBIC = lm(mpg~poly(horsepower,3),data=train)
mean((test$mpg - predict(model_CUBIC, test))^2)
test <- poly(horsepower,2)
test <- poly(test$horsepower,2)
?poly
?cv.glm
?cov
?boot
?rnorm
test.dat <- rnorm(50*2)
x <- matrix(rnorm(50 * 2), ncol = 2)
View(x)
View(x)
?kmeans
?cutree
?scale
?as.dist
?table
?cuttree
?cutree
?table
ppois(3, lambda = 2.5 * 4)
ppois(20, lambda = 16.5 * 2)
pnorm(.51, mean = 0.5, sd = sqrt(1 / 12 / 100), lower.tail = FALSE)
pnorm(.51, mean = 0.5, sd = sqrt(1 / 12 / 100))
?prorm
?pnorm
?t.test
?qt
?t.test
library(RJSONIO)
library(RJSONIO)
json.test <- "{
"data": {
"translations": [
{
"translatedText": "Hallo Welt"
},
{
"translatedText": "Mein Name ist Jeff"
}
]
}
}"
}'
library(RJSONIO)
json.test <- '{
"data": {
"translations": [
{
"translatedText": "Hallo Welt"
},
{
"translatedText": "Mein Name ist Jeff"
}
]
}
}'
data.tes <- fromJSON(json.test)
data.test <- fromJSON(json.test)
data.test[[1]]
data.test[[1]][1]
data.test[[1]][[1]]
data.test[[1]][[1]][1]
data.test[[1]][[1]][2]
data.test[[1]][[1]][1]
library(RJSONIO)
json.test <- '{
"data": {
"translations": [
{
"translatedText": "Hallo Welt"
},
{
"detectedSourceLanguage ": "de"
}
]
}
}'
data.test <- fromJSON(json.test)
data.test[[1]]
data.test[[1]][[1]]
data.test[[1]][[1]][1]
data.test[[1]][[1]][2]
data.test[[1]][[1]][[2]]
data.test[[1]][[1]][1]
data.test[[1]][[1]][2]
data.test[[1]][[1]][1]
data.test[[1]][1]
data.test[[1]][[1]][[1]][1]
data.test[[1]][[1]][[1]][2]
data.test[[1]][[1]][2]
data.test[[1]][[1]][2]
data.test[[1]][2]
?randomForest
install.packages("randomForest")
install.packages("randomForest")
?randomForest
??randomForest
library(randomForest)
??randomForest
?randomForest
library(ISLR)
set.seed(1)
train <- sample(1:nrow(Carseats), nrow(Carseats) / 2)
Carseats.train <- Carseats[train, ]
Carseats.test <- Carseats[-train, ]
library(ISLR)
set.seed(1)
train <- sample(1:nrow(Carseats), nrow(Carseats) / 2)
Carseats.train <- Carseats[train, ]
Carseats.test <- Carseats[-train, ]
library(tree)
tree.carseats <- tree(Sales ~ ., data = Carseats.train)
summary(tree.carseats)
install.packages("tree")
library(ISLR)
set.seed(1)
train <- sample(1:nrow(Carseats), nrow(Carseats) / 2)
Carseats.train <- Carseats[train, ]
Carseats.test <- Carseats[-train, ]
library(tree)
tree.carseats <- tree(Sales ~ ., data = Carseats.train)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty = 0)
cv.carseats <- cv.tree(tree.carseats)
plot(cv.carseats$size, cv.carseats$dev, type = "b")
tree.min <- which.min(cv.carseats$dev)
points(tree.min, cv.carseats$dev[tree.min], col = "red", cex = 2, pch = 20)
summary(cv.carseats)
library(ISLR)
data("Caravan")
setwd('C:\Users\fs651\Documents\GitHub\R-Python-Labs\data')
setwd('C:/Users/fs651/Documents/GitHub/R-Python-Labs/data')
write.csv(Caravan, "Caravan.csv")
?Caravan
View(Caravan)
View(Caravan)
data("Weekly")
write.csv(Weekly, "Weekly.csv")
View(Weekly)
data(OJ)
OJ
View(OJ)
View(Weekly)
View(OJ)
